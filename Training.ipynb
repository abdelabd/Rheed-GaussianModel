{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables: \n",
    "\n",
    "YES = 1\n",
    "NO = 0\n",
    "\n",
    "# Google Drive:\n",
    "using_google_drive = NO\n",
    "\n",
    "if using_google_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m least_squares\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed, compute\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import least_squares\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import hls4ml\n",
    "\n",
    "# %matplotlib inline\n",
    "output_scaler = StandardScaler()\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Normalized Images Shape]: (150985, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "# Read H5 Data File:\n",
    "DATA_DIR = 'data' # Change to your DATA PATH\n",
    "\n",
    "RHEED_DATA_FILE = DATA_DIR + '/RHEED_4848_test6.h5'\n",
    "spot = 'spot_2'\n",
    "h5 = h5py.File(RHEED_DATA_FILE, 'r')\n",
    "\n",
    "raw_data = []\n",
    "for growth in h5.keys():\n",
    "    raw_data.extend(h5[growth][spot])\n",
    "raw_data = np.array(raw_data).astype(np.float32)\n",
    "\n",
    "normalized_images = []\n",
    "for image in raw_data:\n",
    "    normalized_images.append(image / np.max(image))\n",
    "normalized_images = np.array(normalized_images).astype(np.float32)\n",
    "\n",
    "print(f'[Normalized Images Shape]: {normalized_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Normalized Image #43567]:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validate Data Array:\n",
    "validate_data_array = YES\n",
    "\n",
    "if validate_data_array:\n",
    "    rand_int = np.random.randint(low=0, high=normalized_images.shape[0])\n",
    "    print(f'[Normalized Image #{rand_int}]:')\n",
    "    plt.imshow(normalized_images[rand_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for estimating labels\n",
    "\n",
    "# generate 2d Gaussian from its parameters\n",
    "# x, y = x-coord, y-coord\n",
    "# A = amplitude\n",
    "# x0, y0 = mean-x, mean-y\n",
    "# sigma_x, sigma_y = std.-dev.-x, std.-dev.-y\n",
    "def gaussian_2D(x, y, A, x0, y0, sigma_x, sigma_y):\n",
    "    return A * np.exp(-((x - x0)**2 / (2 * sigma_x**2) + (y - y0)**2 / (2 * sigma_y**2)))\n",
    "\n",
    "# Initial guess for each parameter\n",
    "# data = normalized image\n",
    "def add_guess(data):\n",
    "    A_guess = np.max(data)\n",
    "    x0_guess, y0_guess = np.unravel_index(np.argmax(data), data.shape)\n",
    "    sigma_x_guess = sigma_y_guess = np.std(data)\n",
    "    return [A_guess, x0_guess, y0_guess, sigma_x_guess, sigma_y_guess]\n",
    "\n",
    "# Compute residuals\n",
    "# params = A, x0, y0, sigma_x, sigma_y\n",
    "# x, y  = x-coord, y-coord\n",
    "# data = normalized image\n",
    "def residuals(params, x, y, data):\n",
    "    A, x0, y0, sigma_x, sigma_y = params\n",
    "    model = gaussian_2D(x, y, A, x0, y0, sigma_x, sigma_y)\n",
    "    return (model - data).ravel()\n",
    "\n",
    "# Convert parameters from A, x0, y0, sigma_x, sigma_y --> mean_x, mean_y, cov_x, cov_y, theta\n",
    "# params = A, x0, y0, sigma_x, sigma_y\n",
    "def convert_parameters(parameters):\n",
    "    A, x0, y0, sigma_x, sigma_y = parameters\n",
    "    mean_x = x0\n",
    "    mean_y = y0\n",
    "    cov_x = sigma_x\n",
    "    cov_y = sigma_y\n",
    "\n",
    "    if cov_x != 0 and cov_y != 0:\n",
    "        theta = 0.5 * np.arctan(2 * cov_x * cov_y / (cov_x**2 - cov_y**2)+1e-9)\n",
    "    else:\n",
    "        theta = 0.0\n",
    "\n",
    "    return mean_x, mean_y, cov_x, cov_y, theta\n",
    "\n",
    "@delayed\n",
    "def fit_gaussian_2D_delayed(data, guess):\n",
    "    y, x = np.indices(data.shape)\n",
    "    result = least_squares(residuals, guess, args=(x, y, data))\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Estimated Labels Shape]: (150985, 5)\n"
     ]
    }
   ],
   "source": [
    "# Estimate Labels:\n",
    "load_labels = YES # (Takes <1 min to load, ~40 mins to generate)\n",
    "\n",
    "# Import From File\n",
    "if load_labels:\n",
    "    file_path = ''\n",
    "    # estimated_labels = np.load(file_path)\n",
    "    estimated_labels = np.random.rand(normalized_images.shape[0], 5)\n",
    "\n",
    "# Generate\n",
    "else:\n",
    "    estimated_labels = []\n",
    "    with Client() as client:\n",
    "        guesses = [add_guess(image) for image in normalized_images]\n",
    "        fits = [fit_gaussian_2D_delayed(image, guess) for image, guess in zip(normalized_images, guesses)]\n",
    "        estimated_labels = [convert_parameters(params) for params in compute(*fits)]\n",
    "    estimated_labels = np.array(estimated_labels).astype(np.float32)\n",
    "\n",
    "print(f'[Estimated Labels Shape]: {estimated_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 14:56:27.948324: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataSet:\n",
    "batch_size = 1000\n",
    "\n",
    "with tf.device('CPU'):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(normalized_images)\n",
    "    dataset = dataset.shuffle(normalized_images.shape[0], reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "output_scaler.fit(estimated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Function: (TENSORFLOW)\n",
    "print_example_guassian = NO\n",
    "\n",
    "# mean_x, mean_y, cov_x, cov_y, theta\n",
    "def generate_guassian(batch, image_shape):\n",
    "    batch_size = batch.shape[0]\n",
    "    mean_x, mean_y, cov_x, cov_y, theta = tf.unstack(batch, axis=-1)\n",
    "    x = tf.range(image_shape[1], dtype=tf.float32)[:, tf.newaxis]\n",
    "    x = tf.tile(x, [1, image_shape[0]])\n",
    "\n",
    "    y = tf.range(image_shape[0], dtype=tf.float32)[tf.newaxis, :]\n",
    "    y = tf.tile(y, [image_shape[1], 1])\n",
    "\n",
    "    x = tf.tile(tf.expand_dims(x, 0), [batch_size, 1, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [batch_size, 1, 1])\n",
    "\n",
    "    rota_matrix = tf.stack([tf.cos(theta), -tf.sin(theta), tf.sin(theta), tf.cos(theta)], axis=-1)\n",
    "    rota_matrix = tf.reshape(rota_matrix, (batch_size, 2, 2))\n",
    "\n",
    "    xy = tf.stack([x - tf.reshape(mean_x, (-1, 1, 1)), y - tf.reshape(mean_y, (-1, 1, 1))], axis=-1)\n",
    "    xy = tf.einsum('bijk,bkl->bijl', xy, rota_matrix)\n",
    "\n",
    "    img = tf.exp(-0.5 * (xy[:, :, :, 0]**2 / tf.reshape(cov_x, (-1, 1, 1))**2 + xy[:, :, :, 1]**2 / tf.reshape(cov_y, (-1, 1, 1))**2))\n",
    "\n",
    "    return tf.expand_dims(img, axis=1)\n",
    "\n",
    "if print_example_guassian:\n",
    "    image_shape = (48, 48)\n",
    "    batch = tf.convert_to_tensor([\n",
    "        [21.8558168, 24.50041009, 10.31268177, 9.1700225, 0.72681534]\n",
    "        , [21.76068143, 24.37956637, 10.30043488, 9.15426013, 0.72655111]\n",
    "        , [21.72363929, 24.31050759, 10.33800891, 9.18570812, 0.72644599]\n",
    "        , [21.72777699, 24.29306623, 10.30178808, 9.14728058, 0.72610718]\n",
    "        , [21.79849472, 24.34649405, 10.32683150, 9.16259293, 0.72573213]\n",
    "    ])\n",
    "    generated_imgs = generate_guassian(batch, image_shape)\n",
    "    plt.imshow(tf.squeeze(generated_imgs[0]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Custom Weighted MSE Loss]: 3.7529922\n"
     ]
    }
   ],
   "source": [
    "# Custom Loss Function (TENSORFLOW):\n",
    "print_example_loss = YES\n",
    "\n",
    "def custom_weighted_mse_loss(I, J, n):\n",
    "  W = tf.pow(I, n)\n",
    "\n",
    "  squared_diffs = tf.pow(I - J, 2)\n",
    "\n",
    "  weighted_squared_diffs = W * squared_diffs\n",
    "\n",
    "  loss = tf.reduce_mean(weighted_squared_diffs)\n",
    "\n",
    "  return loss\n",
    "\n",
    "if print_example_loss:\n",
    "  I = tf.random.normal((5, 1, 48, 48))\n",
    "  J = tf.random.normal((5, 1, 48, 48))\n",
    "  n = 2\n",
    "  loss = custom_weighted_mse_loss(I, J, n)\n",
    "  print(\"[Custom Weighted MSE Loss]:\", loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large model (won't convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [   \n",
    "        tf.keras.layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='valid', input_shape=(48, 48, 1)) # (batch_size, height, width, channels)\n",
    "        , tf.keras.layers.BatchNormalization(axis=1, momentum=0.1, epsilon=1e-05)\n",
    "        , tf.keras.layers.ReLU()\n",
    "        , tf.keras.layers.MaxPool2D(pool_size=4, strides=4)\n",
    "\n",
    "        , tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid')\n",
    "        , tf.keras.layers.BatchNormalization(axis=1, momentum=0.1, epsilon=1e-05)\n",
    "        , tf.keras.layers.ReLU()\n",
    "        , tf.keras.layers.MaxPool2D(pool_size=2, strides=2)\n",
    "\n",
    "        , tf.keras.layers.Flatten()\n",
    "        , tf.keras.layers.Dense(units=98, activation='relu')\n",
    "        , tf.keras.layers.Dense(units=52, activation='relu')\n",
    "        , tf.keras.layers.Dense(units=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_weighted_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [01:46<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.07773980616733728\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 44, 44, 6)         156       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 44, 44, 6)        176       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 44, 44, 6)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 11, 11, 6)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 7, 16)          2416      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 7, 7, 16)         28        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 144)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 98)                14210     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 52)                5148      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 265       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,399\n",
      "Trainable params: 22,297\n",
      "Non-trainable params: 102\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "train_model = YES\n",
    "save_model = YES\n",
    "load_model = YES\n",
    "\n",
    "if train_model:\n",
    "    best_loss = float('inf')\n",
    "    num_epochs = 1\n",
    "    lr = 0.0001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    n = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            n += 0.1\n",
    "\n",
    "        for image_batch in tqdm(dataset): \n",
    "            image_batch = tf.expand_dims(image_batch, axis=3) # (batch_size, height, width, channels)\n",
    "            with tf.GradientTape() as tape:\n",
    "                embedding = model(image_batch)\n",
    "                unscaled_param = tf.constant(embedding * output_scaler.var_ ** 0.5 + output_scaler.mean_)\n",
    "                final = generate_guassian(unscaled_param, (48,48))\n",
    "                loss = custom_weighted_mse_loss(image_batch, final, n)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            running_loss += loss.numpy()\n",
    "        average_loss = running_loss / len(dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"GaussianModel.keras\")\n",
    "    # pass\n",
    "\n",
    "if save_model:\n",
    "    model.save(\"GaussianModel.keras\")\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv2d_2_input, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "Layer name: conv2d_2, layer type: Conv2D, input shapes: [[None, 48, 48, 1]], output shape: [None, 44, 44, 6]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: re_lu_2, layer type: Activation, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: max_pooling2d_2, layer type: MaxPooling2D, input shapes: [[None, 44, 44, 6]], output shape: [None, 11, 11, 6]\n",
      "Layer name: conv2d_3, layer type: Conv2D, input shapes: [[None, 11, 11, 6]], output shape: [None, 7, 7, 16]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, input shapes: [[None, 7, 7, 16]], output shape: [None, 7, 7, 16]\n",
      "Layer name: re_lu_3, layer type: Activation, input shapes: [[None, 7, 7, 16]], output shape: [None, 7, 7, 16]\n",
      "Layer name: max_pooling2d_3, layer type: MaxPooling2D, input shapes: [[None, 7, 7, 16]], output shape: [None, 3, 3, 16]\n",
      "Layer name: flatten_1, layer type: Reshape, input shapes: [[None, 3, 3, 16]], output shape: [None, 144]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 144]], output shape: [None, 98]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 98]], output shape: [None, 52]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 52]], output shape: [None, 5]\n",
      "{'Model': {'Precision': {'default': 'fixed<16,6>'}, 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}}\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv2d_2_input, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "Layer name: conv2d_2, layer type: Conv2D, input shapes: [[None, 48, 48, 1]], output shape: [None, 44, 44, 6]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: re_lu_2, layer type: Activation, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: max_pooling2d_2, layer type: MaxPooling2D, input shapes: [[None, 44, 44, 6]], output shape: [None, 11, 11, 6]\n",
      "Layer name: conv2d_3, layer type: Conv2D, input shapes: [[None, 11, 11, 6]], output shape: [None, 7, 7, 16]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, input shapes: [[None, 7, 7, 16]], output shape: [None, 7, 7, 16]\n",
      "Layer name: re_lu_3, layer type: Activation, input shapes: [[None, 7, 7, 16]], output shape: [None, 7, 7, 16]\n",
      "Layer name: max_pooling2d_3, layer type: MaxPooling2D, input shapes: [[None, 7, 7, 16]], output shape: [None, 3, 3, 16]\n",
      "Layer name: flatten_1, layer type: Reshape, input shapes: [[None, 3, 3, 16]], output shape: [None, 144]\n",
      "Layer name: dense_3, layer type: Dense, input shapes: [[None, 144]], output shape: [None, 98]\n",
      "Layer name: dense_4, layer type: Dense, input shapes: [[None, 98]], output shape: [None, 52]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 52]], output shape: [None, 5]\n",
      "Creating HLS model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (44,) (5,5,1,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Attempt conversion on simplified model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m hls_model \u001b[38;5;241m=\u001b[39m \u001b[43mhls4ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_from_keras_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhls_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_1/hls4ml_prj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxcu250-figd2104-2L-e\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/converters/__init__.py:238\u001b[0m, in \u001b[0;36mconvert_from_keras_model\u001b[0;34m(model, output_dir, project_name, input_data_tb, output_data_tb, backend, hls_config, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHLSConfig\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m _check_model_config(model_config)\n\u001b[1;32m    236\u001b[0m _check_hls_config(config, hls_config)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras_to_hls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/converters/keras_to_hls.py:337\u001b[0m, in \u001b[0;36mkeras_to_hls\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    335\u001b[0m layer_list, input_layers, output_layers, _ \u001b[38;5;241m=\u001b[39m parse_keras_model(model_arch, reader)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating HLS model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m hls_model \u001b[38;5;241m=\u001b[39m \u001b[43mModelGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hls_model\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:390\u001b[0m, in \u001b[0;36mModelGraph.__init__\u001b[0;34m(self, config, layer_list, inputs, outputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_graph(layer_list)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flow \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mflows:\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:452\u001b[0m, in \u001b[0;36mModelGraph.apply_flow\u001b[0;34m(self, flow, reapply)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_applied_flows\u001b[38;5;241m.\u001b[39mappend(applied_flows)\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:461\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_flow \u001b[38;5;129;01min\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mrequires:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sub_flow \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m applied_flows\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 461\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    464\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m optimize_model(\u001b[38;5;28mself\u001b[39m, flow\u001b[38;5;241m.\u001b[39moptimizers)\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:461\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_flow \u001b[38;5;129;01min\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mrequires:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sub_flow \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m applied_flows\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 461\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    464\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m optimize_model(\u001b[38;5;28mself\u001b[39m, flow\u001b[38;5;241m.\u001b[39moptimizers)\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:464\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_sub_flow(sub_flow, applied_flows)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 464\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/optimizer/optimizer.py:318\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(model, passes)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mmatch(node):\n\u001b[0;32m--> 318\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m         applied_passes\u001b[38;5;241m.\u001b[39madd(opt_name)\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m res:\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/optimizer/passes/bn_fuse.py:29\u001b[0m, in \u001b[0;36mFuseBatchNormalization.transform\u001b[0;34m(self, model, node)\u001b[0m\n\u001b[1;32m     26\u001b[0m bn_scale \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m bn_bias \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m fused_weight \u001b[38;5;241m=\u001b[39m \u001b[43mbn_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparent_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     30\u001b[0m fused_bias \u001b[38;5;241m=\u001b[39m bn_scale\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m parent_bias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m bn_bias\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mremove_node(node, rewire\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (44,) (5,5,1,6) "
     ]
    }
   ],
   "source": [
    "# Generate the configuration from the Keras model\n",
    "config = hls4ml.utils.config_from_keras_model(model)\n",
    "\n",
    "# Print out the config to debug and check for unwanted settings\n",
    "print(config)\n",
    "\n",
    "# Attempt conversion on simplified model\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_1/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(csim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller model, does convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='valid', input_shape=(48, 48, 1)),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=4, strides=4),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=98, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=5, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.compile(optimizer='adam', loss=custom_weighted_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:21<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.07774685555143862\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 44, 44, 6)         156       \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 44, 44, 6)         0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 11, 11, 6)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 726)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 98)                71246     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 495       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,897\n",
      "Trainable params: 71,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "train_model = YES\n",
    "save_model = YES\n",
    "load_model = NO\n",
    "\n",
    "if train_model:\n",
    "    best_loss = float('inf')\n",
    "    num_epochs = 1\n",
    "    lr = 0.0001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    n = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            n += 0.1\n",
    "\n",
    "        for image_batch in tqdm(dataset): \n",
    "            image_batch = tf.expand_dims(image_batch, axis=3) # (batch_size, height, width, channels)\n",
    "            with tf.GradientTape() as tape:\n",
    "                embedding = model2(image_batch)\n",
    "                unscaled_param = tf.constant(embedding * output_scaler.var_ ** 0.5 + output_scaler.mean_)\n",
    "                final = generate_guassian(unscaled_param, (48,48))\n",
    "                loss = custom_weighted_mse_loss(image_batch, final, n)\n",
    "            grads = tape.gradient(loss, model2.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model2.trainable_variables))\n",
    "\n",
    "            running_loss += loss.numpy()\n",
    "        average_loss = running_loss / len(dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "\n",
    "if save_model:\n",
    "    model2.save(\"GaussianModel2.keras\")\n",
    "\n",
    "if load_model:\n",
    "    pass\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv2d_4_input, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "Layer name: conv2d_4, layer type: Conv2D, input shapes: [[None, 48, 48, 1]], output shape: [None, 44, 44, 6]\n",
      "Layer name: re_lu_4, layer type: Activation, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: max_pooling2d_4, layer type: MaxPooling2D, input shapes: [[None, 44, 44, 6]], output shape: [None, 11, 11, 6]\n",
      "Layer name: flatten_2, layer type: Reshape, input shapes: [[None, 11, 11, 6]], output shape: [None, 726]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 726]], output shape: [None, 98]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 98]], output shape: [None, 5]\n",
      "{'Model': {'Precision': {'default': 'fixed<16,6>'}, 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}}\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv2d_4_input, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "Layer name: conv2d_4, layer type: Conv2D, input shapes: [[None, 48, 48, 1]], output shape: [None, 44, 44, 6]\n",
      "Layer name: re_lu_4, layer type: Activation, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: max_pooling2d_4, layer type: MaxPooling2D, input shapes: [[None, 44, 44, 6]], output shape: [None, 11, 11, 6]\n",
      "Layer name: flatten_2, layer type: Reshape, input shapes: [[None, 11, 11, 6]], output shape: [None, 726]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 726]], output shape: [None, 98]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 98]], output shape: [None, 5]\n",
      "Creating HLS model\n",
      "WARNING: Layer conv2d_4 requires \"dataflow\" pipeline style. Switching to \"dataflow\" pipeline style.\n"
     ]
    }
   ],
   "source": [
    "# Generate the configuration from the Keras model\n",
    "config2 = hls4ml.utils.config_from_keras_model(model2)\n",
    "\n",
    "# Print out the config to debug and check for unwanted settings\n",
    "print(config2)\n",
    "\n",
    "# Attempt conversion on simplified model\n",
    "hls_model2 = hls4ml.converters.convert_from_keras_model(\n",
    "    model2, hls_config=config2, output_dir='model_2/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "Done\n",
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)\n",
      "  **** SW Build 2552052 on Fri May 24 14:47:09 MDT 2019\n",
      "  **** IP Build 2548770 on Fri May 24 18:01:18 MDT 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /tools/Xilinx/Vivado/2019.1/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] Running '/tools/Xilinx/Vivado/2019.1/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "INFO: [HLS 200-10] For user 'aelabd' on host 'DESKTOP-Q0UCNGC.' (Linux_x86_64 version 5.15.133.1-microsoft-standard-WSL2) on Thu Nov 21 14:55:29 PST 2024\n",
      "INFO: [HLS 200-10] On os Ubuntu 24.04 LTS\n",
      "INFO: [HLS 200-10] In directory '/home/aelabd/RHEED/Rheed-GaussianModel/model_2/hls4ml_prj'\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-10] Creating and opening project '/home/aelabd/RHEED/Rheed-GaussianModel/model_2/hls4ml_prj/myproject_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Creating and opening solution '/home/aelabd/RHEED/Rheed-GaussianModel/model_2/hls4ml_prj/myproject_prj/solution1'.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "INFO: [HLS 200-10] Setting target device to 'xcu250-figd2104-2L-e'\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.625ns.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:38:75\n",
      "WARNING: [HLS 200-471] Dataflow form checks found 1 issue(s) in file firmware/myproject.cpp\n"
     ]
    }
   ],
   "source": [
    "hls_model2.build(csim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other large model\n",
    "Same architecture as first large model but not tf.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 48, 48, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 44, 44, 6)         156       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 44, 44, 6)        176       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_7 (ReLU)              (None, 44, 44, 6)         0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 11, 11, 6)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 7, 7, 16)          2416      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 7, 7, 16)         28        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_8 (ReLU)              (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 3, 3, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 144)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 98)                14210     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 52)                5148      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 265       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,399\n",
      "Trainable params: 22,297\n",
      "Non-trainable params: 102\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class CropLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CropLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        image, crop_boxes = inputs\n",
    "        batch_size = tf.shape(image)[0]\n",
    "        num_boxes = tf.shape(crop_boxes)[1]\n",
    "        crop_boxes = tf.reshape(crop_boxes, (batch_size * num_boxes, 4))  # Flatten the crop_boxes tensor\n",
    "        box_indices = tf.range(batch_size)\n",
    "        box_indices = tf.repeat(box_indices, repeats=num_boxes)\n",
    "        \n",
    "        cropped_images = tf.image.crop_and_resize(\n",
    "            image, crop_boxes, box_indices, (20, 20)\n",
    "        )\n",
    "        \n",
    "        return cropped_images\n",
    "\n",
    "def build_cnn_with_gaussian_prediction(input_shape):\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "   \n",
    "    # Initial convolutions to process the full image\n",
    "    x = layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='valid')(inputs) # (batch_size, height, width, channels)\n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.1, epsilon=1e-05)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(pool_size=4, strides=4)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid')(x)\n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.1, epsilon=1e-05)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(pool_size=2, strides=2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=98, activation='relu')(x)\n",
    "    x = layers.Dense(units=52, activation='relu')(x)\n",
    "    x = layers.Dense(units=5)(x)\n",
    "\n",
    "    #got rid of crop_pred output\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (48, 48, 1)\n",
    "model3 = build_cnn_with_gaussian_prediction(input_shape)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [01:40<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.07773976708879533\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 48, 48, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 44, 44, 6)         156       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 44, 44, 6)        176       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_7 (ReLU)              (None, 44, 44, 6)         0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 11, 11, 6)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 7, 7, 16)          2416      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 7, 7, 16)         28        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_8 (ReLU)              (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 3, 3, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 144)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 98)                14210     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 52)                5148      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 265       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,399\n",
      "Trainable params: 22,297\n",
      "Non-trainable params: 102\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "train_model = YES\n",
    "save_model = YES\n",
    "load_model = NO\n",
    "\n",
    "if train_model:\n",
    "    best_loss = float('inf')\n",
    "    num_epochs = 1\n",
    "    lr = 0.0001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    n = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            n += 0.1\n",
    "\n",
    "        for image_batch in tqdm(dataset): \n",
    "            image_batch = tf.expand_dims(image_batch, axis=3) # (batch_size, height, width, channels)\n",
    "            with tf.GradientTape() as tape:\n",
    "                embedding = model3(image_batch)\n",
    "                unscaled_param = tf.constant(embedding * output_scaler.var_ ** 0.5 + output_scaler.mean_)\n",
    "                final = generate_guassian(unscaled_param, (48,48))\n",
    "                loss = custom_weighted_mse_loss(image_batch, final, n)\n",
    "            grads = tape.gradient(loss, model3.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model3.trainable_variables))\n",
    "\n",
    "            running_loss += loss.numpy()\n",
    "        average_loss = running_loss / len(dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "\n",
    "if save_model:\n",
    "    model3.save(\"GaussianModel3.keras\")\n",
    "\n",
    "if load_model:\n",
    "    pass\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_2, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "Layer name: conv2d_7, layer type: Conv2D, input shapes: [[None, 48, 48, 1]], output shape: [None, 44, 44, 6]\n",
      "Layer name: batch_normalization_6, layer type: BatchNormalization, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: re_lu_7, layer type: Activation, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: max_pooling2d_7, layer type: MaxPooling2D, input shapes: [[None, 44, 44, 6]], output shape: [None, 11, 11, 6]\n",
      "Layer name: conv2d_8, layer type: Conv2D, input shapes: [[None, 11, 11, 6]], output shape: [None, 7, 7, 16]\n",
      "Layer name: batch_normalization_7, layer type: BatchNormalization, input shapes: [[None, 7, 7, 16]], output shape: [None, 7, 7, 16]\n",
      "Layer name: re_lu_8, layer type: Activation, input shapes: [[None, 7, 7, 16]], output shape: [None, 7, 7, 16]\n",
      "Layer name: max_pooling2d_8, layer type: MaxPooling2D, input shapes: [[None, 7, 7, 16]], output shape: [None, 3, 3, 16]\n",
      "Layer name: flatten_4, layer type: Reshape, input shapes: [[None, 3, 3, 16]], output shape: [None, 144]\n",
      "Layer name: dense_11, layer type: Dense, input shapes: [[None, 144]], output shape: [None, 98]\n",
      "Layer name: dense_12, layer type: Dense, input shapes: [[None, 98]], output shape: [None, 52]\n",
      "Layer name: dense_13, layer type: Dense, input shapes: [[None, 52]], output shape: [None, 5]\n",
      "{'Model': {'Precision': {'default': 'fixed<16,6>'}, 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}}\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_2, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "Layer name: conv2d_7, layer type: Conv2D, input shapes: [[None, 48, 48, 1]], output shape: [None, 44, 44, 6]\n",
      "Layer name: batch_normalization_6, layer type: BatchNormalization, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: re_lu_7, layer type: Activation, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: max_pooling2d_7, layer type: MaxPooling2D, input shapes: [[None, 44, 44, 6]], output shape: [None, 11, 11, 6]\n",
      "Layer name: conv2d_8, layer type: Conv2D, input shapes: [[None, 11, 11, 6]], output shape: [None, 7, 7, 16]\n",
      "Layer name: batch_normalization_7, layer type: BatchNormalization, input shapes: [[None, 7, 7, 16]], output shape: [None, 7, 7, 16]\n",
      "Layer name: re_lu_8, layer type: Activation, input shapes: [[None, 7, 7, 16]], output shape: [None, 7, 7, 16]\n",
      "Layer name: max_pooling2d_8, layer type: MaxPooling2D, input shapes: [[None, 7, 7, 16]], output shape: [None, 3, 3, 16]\n",
      "Layer name: flatten_4, layer type: Reshape, input shapes: [[None, 3, 3, 16]], output shape: [None, 144]\n",
      "Layer name: dense_11, layer type: Dense, input shapes: [[None, 144]], output shape: [None, 98]\n",
      "Layer name: dense_12, layer type: Dense, input shapes: [[None, 98]], output shape: [None, 52]\n",
      "Layer name: dense_13, layer type: Dense, input shapes: [[None, 52]], output shape: [None, 5]\n",
      "Creating HLS model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (44,) (5,5,1,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(config3)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Attempt conversion on simplified model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m hls_model3 \u001b[38;5;241m=\u001b[39m \u001b[43mhls4ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_from_keras_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhls_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_3/hls4ml_prj\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxcu250-figd2104-2L-e\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/converters/__init__.py:238\u001b[0m, in \u001b[0;36mconvert_from_keras_model\u001b[0;34m(model, output_dir, project_name, input_data_tb, output_data_tb, backend, hls_config, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHLSConfig\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m _check_model_config(model_config)\n\u001b[1;32m    236\u001b[0m _check_hls_config(config, hls_config)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras_to_hls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/converters/keras_to_hls.py:337\u001b[0m, in \u001b[0;36mkeras_to_hls\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    335\u001b[0m layer_list, input_layers, output_layers, _ \u001b[38;5;241m=\u001b[39m parse_keras_model(model_arch, reader)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating HLS model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m hls_model \u001b[38;5;241m=\u001b[39m \u001b[43mModelGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hls_model\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:390\u001b[0m, in \u001b[0;36mModelGraph.__init__\u001b[0;34m(self, config, layer_list, inputs, outputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_graph(layer_list)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flow \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mflows:\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:452\u001b[0m, in \u001b[0;36mModelGraph.apply_flow\u001b[0;34m(self, flow, reapply)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_applied_flows\u001b[38;5;241m.\u001b[39mappend(applied_flows)\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:461\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_flow \u001b[38;5;129;01min\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mrequires:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sub_flow \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m applied_flows\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 461\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    464\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m optimize_model(\u001b[38;5;28mself\u001b[39m, flow\u001b[38;5;241m.\u001b[39moptimizers)\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:461\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_flow \u001b[38;5;129;01min\u001b[39;00m flow\u001b[38;5;241m.\u001b[39mrequires:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sub_flow \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m applied_flows\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 461\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_flow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    464\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m optimize_model(\u001b[38;5;28mself\u001b[39m, flow\u001b[38;5;241m.\u001b[39moptimizers)\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/graph.py:464\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_sub_flow(sub_flow, applied_flows)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 464\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/optimizer/optimizer.py:318\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(model, passes)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mmatch(node):\n\u001b[0;32m--> 318\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m         applied_passes\u001b[38;5;241m.\u001b[39madd(opt_name)\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m res:\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/model/optimizer/passes/bn_fuse.py:29\u001b[0m, in \u001b[0;36mFuseBatchNormalization.transform\u001b[0;34m(self, model, node)\u001b[0m\n\u001b[1;32m     26\u001b[0m bn_scale \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m bn_bias \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m fused_weight \u001b[38;5;241m=\u001b[39m \u001b[43mbn_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparent_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     30\u001b[0m fused_bias \u001b[38;5;241m=\u001b[39m bn_scale\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m parent_bias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m bn_bias\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mremove_node(node, rewire\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (44,) (5,5,1,6) "
     ]
    }
   ],
   "source": [
    "# Generate the configuration from the Keras model\n",
    "config3 = hls4ml.utils.config_from_keras_model(model3)\n",
    "\n",
    "# Print out the config to debug and check for unwanted settings\n",
    "print(config3)\n",
    "\n",
    "# Attempt conversion on simplified model\n",
    "hls_model3 = hls4ml.converters.convert_from_keras_model(\n",
    "    model3, hls_config=config3, output_dir='model_3/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model3.build(csim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some benchmarking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Up for Benchmarking:\n",
    "avg_params = np.mean(estimated_labels, axis=0) # output_scaler.mean_ ?\n",
    "std_params = np.std(estimated_labels, axis=0) # output_scaler.var_ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generated Gaussian Labels Shape]: (1000, 5)\n",
      "[Generated Guassian Images Shape]: (1000, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "# Generate Plausible Gaussians:\n",
    "num_generated_gaussians = 1000\n",
    "\n",
    "generated_gaussians_labels = []\n",
    "for num in range(num_generated_gaussians):\n",
    "    new_gaussian_label = []\n",
    "    for (avg, std) in zip(avg_params, std_params):\n",
    "        new_gaussian_label.append(avg + np.random.normal(loc=0, scale=std))\n",
    "    generated_gaussians_labels.append(new_gaussian_label)\n",
    "generated_gaussians_labels = np.array(generated_gaussians_labels)\n",
    "\n",
    "generated_gaussians_images = generate_guassian(tf.convert_to_tensor(generated_gaussians_labels, dtype=tf.float32), (48,48))\n",
    "\n",
    "print(f'[Generated Gaussian Labels Shape]: {generated_gaussians_labels.shape}')\n",
    "print(f'[Generated Guassian Images Shape]: {generated_gaussians_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
