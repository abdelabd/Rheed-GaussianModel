{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables: \n",
    "\n",
    "YES = 1\n",
    "NO = 0\n",
    "\n",
    "# Google Drive:\n",
    "using_google_drive = NO\n",
    "\n",
    "if using_google_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 17:49:17.420738: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 17:49:17.611921: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 17:49:17.616067: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:17.616089: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-21 17:49:18.494379: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:18.494555: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:18.494563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/aelabd/RHEED/hls4ml/hls4ml/converters/__init__.py:29: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from pooling.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from recurrent.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from reshape.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from merge.py: No module named 'torch'.\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 17:49:21.603737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-21 17:49:21.603977: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:21.604056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:21.604122: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:21.604191: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:21.604261: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:21.604326: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:21.604392: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:21.604458: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:49:21.604471: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import least_squares\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import hls4ml\n",
    "\n",
    "# %matplotlib inline\n",
    "output_scaler = StandardScaler()\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Normalized Images Shape]: (150985, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "# Read H5 Data File:\n",
    "DATA_DIR = 'data' # Change to your DATA PATH\n",
    "\n",
    "RHEED_DATA_FILE = DATA_DIR + '/RHEED_4848_test6.h5'\n",
    "spot = 'spot_2'\n",
    "h5 = h5py.File(RHEED_DATA_FILE, 'r')\n",
    "\n",
    "raw_data = []\n",
    "for growth in h5.keys():\n",
    "    raw_data.extend(h5[growth][spot])\n",
    "raw_data = np.array(raw_data).astype(np.float32)\n",
    "\n",
    "normalized_images = []\n",
    "for image in raw_data:\n",
    "    normalized_images.append(image / np.max(image))\n",
    "normalized_images = np.array(normalized_images).astype(np.float32)\n",
    "\n",
    "print(f'[Normalized Images Shape]: {normalized_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Normalized Image #43567]:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validate Data Array:\n",
    "validate_data_array = YES\n",
    "\n",
    "if validate_data_array:\n",
    "    rand_int = np.random.randint(low=0, high=normalized_images.shape[0])\n",
    "    print(f'[Normalized Image #{rand_int}]:')\n",
    "    plt.imshow(normalized_images[rand_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for estimating labels\n",
    "\n",
    "# generate 2d Gaussian from its parameters\n",
    "# x, y = x-coord, y-coord\n",
    "# A = amplitude\n",
    "# x0, y0 = mean-x, mean-y\n",
    "# sigma_x, sigma_y = std.-dev.-x, std.-dev.-y\n",
    "def gaussian_2D(x, y, A, x0, y0, sigma_x, sigma_y):\n",
    "    return A * np.exp(-((x - x0)**2 / (2 * sigma_x**2) + (y - y0)**2 / (2 * sigma_y**2)))\n",
    "\n",
    "# Initial guess for each parameter\n",
    "# data = normalized image\n",
    "def add_guess(data):\n",
    "    A_guess = np.max(data)\n",
    "    x0_guess, y0_guess = np.unravel_index(np.argmax(data), data.shape)\n",
    "    sigma_x_guess = sigma_y_guess = np.std(data)\n",
    "    return [A_guess, x0_guess, y0_guess, sigma_x_guess, sigma_y_guess]\n",
    "\n",
    "# Compute residuals\n",
    "# params = A, x0, y0, sigma_x, sigma_y\n",
    "# x, y  = x-coord, y-coord\n",
    "# data = normalized image\n",
    "def residuals(params, x, y, data):\n",
    "    A, x0, y0, sigma_x, sigma_y = params\n",
    "    model = gaussian_2D(x, y, A, x0, y0, sigma_x, sigma_y)\n",
    "    return (model - data).ravel()\n",
    "\n",
    "# Convert parameters from A, x0, y0, sigma_x, sigma_y --> mean_x, mean_y, cov_x, cov_y, theta\n",
    "# params = A, x0, y0, sigma_x, sigma_y\n",
    "def convert_parameters(parameters):\n",
    "    A, x0, y0, sigma_x, sigma_y = parameters\n",
    "    mean_x = x0\n",
    "    mean_y = y0\n",
    "    cov_x = sigma_x\n",
    "    cov_y = sigma_y\n",
    "\n",
    "    if cov_x != 0 and cov_y != 0:\n",
    "        theta = 0.5 * np.arctan(2 * cov_x * cov_y / (cov_x**2 - cov_y**2)+1e-9)\n",
    "    else:\n",
    "        theta = 0.0\n",
    "\n",
    "    return mean_x, mean_y, cov_x, cov_y, theta\n",
    "\n",
    "@delayed\n",
    "def fit_gaussian_2D_delayed(data, guess):\n",
    "    y, x = np.indices(data.shape)\n",
    "    result = least_squares(residuals, guess, args=(x, y, data))\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Estimated Labels Shape]: (150985, 5)\n"
     ]
    }
   ],
   "source": [
    "# Estimate Labels:\n",
    "load_labels = YES # (Takes <1 min to load, ~40 mins to generate)\n",
    "\n",
    "# Import From File\n",
    "if load_labels:\n",
    "    file_path = ''\n",
    "    # estimated_labels = np.load(file_path)\n",
    "    estimated_labels = np.random.rand(normalized_images.shape[0], 5)\n",
    "\n",
    "# Generate\n",
    "else:\n",
    "    estimated_labels = []\n",
    "    with Client() as client:\n",
    "        guesses = [add_guess(image) for image in normalized_images]\n",
    "        fits = [fit_gaussian_2D_delayed(image, guess) for image, guess in zip(normalized_images, guesses)]\n",
    "        estimated_labels = [convert_parameters(params) for params in compute(*fits)]\n",
    "    estimated_labels = np.array(estimated_labels).astype(np.float32)\n",
    "\n",
    "print(f'[Estimated Labels Shape]: {estimated_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 17:49:25.607647: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataSet:\n",
    "batch_size = 1000\n",
    "\n",
    "with tf.device('CPU'):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(normalized_images)\n",
    "    dataset = dataset.shuffle(normalized_images.shape[0], reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "output_scaler.fit(estimated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Function: (TENSORFLOW)\n",
    "print_example_guassian = NO\n",
    "\n",
    "# mean_x, mean_y, cov_x, cov_y, theta\n",
    "def generate_guassian(batch, image_shape):\n",
    "    batch_size = batch.shape[0]\n",
    "    mean_x, mean_y, cov_x, cov_y, theta = tf.unstack(batch, axis=-1)\n",
    "    x = tf.range(image_shape[1], dtype=tf.float32)[:, tf.newaxis]\n",
    "    x = tf.tile(x, [1, image_shape[0]])\n",
    "\n",
    "    y = tf.range(image_shape[0], dtype=tf.float32)[tf.newaxis, :]\n",
    "    y = tf.tile(y, [image_shape[1], 1])\n",
    "\n",
    "    x = tf.tile(tf.expand_dims(x, 0), [batch_size, 1, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [batch_size, 1, 1])\n",
    "\n",
    "    rota_matrix = tf.stack([tf.cos(theta), -tf.sin(theta), tf.sin(theta), tf.cos(theta)], axis=-1)\n",
    "    rota_matrix = tf.reshape(rota_matrix, (batch_size, 2, 2))\n",
    "\n",
    "    xy = tf.stack([x - tf.reshape(mean_x, (-1, 1, 1)), y - tf.reshape(mean_y, (-1, 1, 1))], axis=-1)\n",
    "    xy = tf.einsum('bijk,bkl->bijl', xy, rota_matrix)\n",
    "\n",
    "    img = tf.exp(-0.5 * (xy[:, :, :, 0]**2 / tf.reshape(cov_x, (-1, 1, 1))**2 + xy[:, :, :, 1]**2 / tf.reshape(cov_y, (-1, 1, 1))**2))\n",
    "\n",
    "    return tf.expand_dims(img, axis=1)\n",
    "\n",
    "if print_example_guassian:\n",
    "    image_shape = (48, 48)\n",
    "    batch = tf.convert_to_tensor([\n",
    "        [21.8558168, 24.50041009, 10.31268177, 9.1700225, 0.72681534]\n",
    "        , [21.76068143, 24.37956637, 10.30043488, 9.15426013, 0.72655111]\n",
    "        , [21.72363929, 24.31050759, 10.33800891, 9.18570812, 0.72644599]\n",
    "        , [21.72777699, 24.29306623, 10.30178808, 9.14728058, 0.72610718]\n",
    "        , [21.79849472, 24.34649405, 10.32683150, 9.16259293, 0.72573213]\n",
    "    ])\n",
    "    generated_imgs = generate_guassian(batch, image_shape)\n",
    "    plt.imshow(tf.squeeze(generated_imgs[0]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Custom Weighted MSE Loss]: 4.103805\n"
     ]
    }
   ],
   "source": [
    "# Custom Loss Function (TENSORFLOW):\n",
    "print_example_loss = YES\n",
    "\n",
    "def custom_weighted_mse_loss(I, J, n):\n",
    "  W = tf.pow(I, n)\n",
    "\n",
    "  squared_diffs = tf.pow(I - J, 2)\n",
    "\n",
    "  weighted_squared_diffs = W * squared_diffs\n",
    "\n",
    "  loss = tf.reduce_mean(weighted_squared_diffs)\n",
    "\n",
    "  return loss\n",
    "\n",
    "if print_example_loss:\n",
    "  I = tf.random.normal((5, 1, 48, 48))\n",
    "  J = tf.random.normal((5, 1, 48, 48))\n",
    "  n = 2\n",
    "  loss = custom_weighted_mse_loss(I, J, n)\n",
    "  print(\"[Custom Weighted MSE Loss]:\", loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Train and convert just the Gaussian predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='valid', input_shape=(48, 48, 1)),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=4, strides=4),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=98, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=5, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.compile(optimizer='adam', loss=custom_weighted_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "# train_model = YES\n",
    "# save_model = YES\n",
    "# load_model = NO\n",
    "\n",
    "# if train_model:\n",
    "#     best_loss = float('inf')\n",
    "#     num_epochs = 1\n",
    "#     lr = 0.0001\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#     n = 1\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         if epoch % 10 == 0:\n",
    "#             n += 0.1\n",
    "\n",
    "#         for image_batch in tqdm(dataset): \n",
    "#             image_batch = tf.expand_dims(image_batch, axis=3) # (batch_size, height, width, channels)\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 embedding = model2(image_batch)\n",
    "#                 unscaled_param = tf.constant(embedding * output_scaler.var_ ** 0.5 + output_scaler.mean_)\n",
    "#                 final = generate_guassian(unscaled_param, (48,48))\n",
    "#                 loss = custom_weighted_mse_loss(image_batch, final, n)\n",
    "#             grads = tape.gradient(loss, model2.trainable_variables)\n",
    "#             optimizer.apply_gradients(zip(grads, model2.trainable_variables))\n",
    "\n",
    "#             running_loss += loss.numpy()\n",
    "#         average_loss = running_loss / len(dataset)\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "\n",
    "# if save_model:\n",
    "#     model2.save(\"GaussianModel2.keras\")\n",
    "\n",
    "# if load_model:\n",
    "#     pass\n",
    "\n",
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv2d_input, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "Layer name: conv2d, layer type: Conv2D, input shapes: [[None, 48, 48, 1]], output shape: [None, 44, 44, 6]\n",
      "Layer name: re_lu, layer type: Activation, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, input shapes: [[None, 44, 44, 6]], output shape: [None, 11, 11, 6]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 11, 11, 6]], output shape: [None, 726]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 726]], output shape: [None, 98]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 98]], output shape: [None, 5]\n",
      "{'Model': {'Precision': {'default': 'fixed<16,6>'}, 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}}\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv2d_input, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "Layer name: conv2d, layer type: Conv2D, input shapes: [[None, 48, 48, 1]], output shape: [None, 44, 44, 6]\n",
      "Layer name: re_lu, layer type: Activation, input shapes: [[None, 44, 44, 6]], output shape: [None, 44, 44, 6]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, input shapes: [[None, 44, 44, 6]], output shape: [None, 11, 11, 6]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 11, 11, 6]], output shape: [None, 726]\n",
      "Layer name: dense, layer type: Dense, input shapes: [[None, 726]], output shape: [None, 98]\n",
      "Layer name: dense_1, layer type: Dense, input shapes: [[None, 98]], output shape: [None, 5]\n",
      "Creating HLS model\n",
      "WARNING: Layer conv2d requires \"dataflow\" pipeline style. Switching to \"dataflow\" pipeline style.\n"
     ]
    }
   ],
   "source": [
    "# Generate the configuration from the Keras model\n",
    "config2 = hls4ml.utils.config_from_keras_model(model2)\n",
    "\n",
    "# Print out the config to debug and check for unwanted settings\n",
    "print(config2)\n",
    "\n",
    "# Attempt conversion on simplified model\n",
    "hls_model2 = hls4ml.converters.convert_from_keras_model(\n",
    "    model2, hls_config=config2, output_dir='model_2/hls4ml_prj', part='xcu250-figd2104-2L-e', io_type=\"io_stream\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hls_model2.build(csim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and convert the Gaussian predictor but with a CropLayer on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tensorflow.keras import layers\n",
    "from hls4ml.converters.keras_to_hls import parse_default_keras_layer\n",
    "from hls4ml.model.attributes import Attribute\n",
    "\n",
    "IMAGE_SIZE = (100, 160)\n",
    "NUM_BLOBS = 1\n",
    "CROP_BOX_ROWS = 48\n",
    "CROP_BOX_COLS = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropLayer(layers.Layer):\n",
    "    def __init__(self, image_shape, crop_shape, num_boxes, **kwargs):\n",
    "        super(CropLayer, self).__init__(**kwargs)\n",
    "        self.image_shape = image_shape\n",
    "        self.crop_shape = crop_shape\n",
    "        self.num_boxes = num_boxes\n",
    "\n",
    "    def call(self, inputs):\n",
    "        image = inputs\n",
    "        batch_size = tf.shape(image)[0]\n",
    "\n",
    "        crop_boxes_hc = tf.convert_to_tensor(np.array([[0.38787913, 0.37316015, 0.86787915, 0.67316014]], dtype=np.float32))\n",
    "        crop_boxes = tf.repeat(crop_boxes_hc, batch_size)\n",
    "        # crop_boxes = tf.convert_to_tensor(crop_boxes_hc)\n",
    "        \n",
    "        crop_boxes = tf.reshape(crop_boxes, (batch_size * self.num_boxes, 4))  # Flatten the crop_boxes tensor\n",
    "        box_indices = tf.range(batch_size)\n",
    "        box_indices = tf.repeat(box_indices, repeats=self.num_boxes)\n",
    "        \n",
    "        cropped_images = tf.image.crop_and_resize(\n",
    "            image, crop_boxes, box_indices, self.crop_shape\n",
    "        )\n",
    "        # cropped_images = tf.squeeze(cropped_images)\n",
    "        \n",
    "        return cropped_images\n",
    "    \n",
    "    def get_config(self):\n",
    "        # Breaks serialization and parsing in hls4ml if not defined\n",
    "        return super().get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_crop_layer(keras_layer, input_names, input_shapes, data_reader):\n",
    "    print(f\"input_names = {input_names}\")\n",
    "    print(f\"input_shapes = {input_shapes}\")\n",
    "    \n",
    "    assert 'CropLayer' in keras_layer['class_name']\n",
    "    print(f\"keras_layer['name']: {keras_layer['name']}\")\n",
    "    \n",
    "    crop_shape = None\n",
    "    for l in data_reader.model.layers:\n",
    "        print(f\"\\n l: {l}, \\n l.__dict__: {l.__dict__} \\n\")\n",
    "        try:\n",
    "            if l.name == keras_layer['name']:\n",
    "                print(f\"l{l}\")\n",
    "                crop_shape = l.crop_shape\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if crop_shape is None:\n",
    "        raise AttributeError(f\"Could not find crop_shape attribute for layer {keras_layer['name']}\")\n",
    "\n",
    "    layer = parse_default_keras_layer(keras_layer, input_names)\n",
    "    img_shape = input_shapes[0]\n",
    "\n",
    "    num_boxes = NUM_BLOBS\n",
    "    num_channels = img_shape[-1]\n",
    "    output_shape = [None, num_boxes, crop_shape[0], crop_shape[1], num_channels]\n",
    "\n",
    "    layer[\"num_boxes\"] = num_boxes\n",
    "    layer[\"num_channels\"] = num_channels\n",
    "    layer[\"img_shape\"] = img_shape\n",
    "    max_idx = np.multiply(np.multiply(img_shape[1], img_shape[2]), img_shape[3])\n",
    "    layer[\"idx_t\"] = f\"ap_uint<{int(np.ceil(np.log2(max_idx)) + 1)}>\"\n",
    "\n",
    "    return layer, output_shape\n",
    "\n",
    "class HCropLayer(hls4ml.model.layers.Layer):\n",
    "    \"hls4ml implementation of the CropLayer\"\n",
    "    _expected_attributes = [Attribute('crop_rows'), Attribute('crop_cols')]\n",
    "\n",
    "    def initialize(self):\n",
    "        inp_0 = self.model.get_layer_output_variable(self.inputs[0]) # Image\n",
    "\n",
    "        crop_shape = None\n",
    "        for l in self.model.config.config['KerasModel'].layers:\n",
    "            if l.name == self.get_attr('name'):\n",
    "                crop_shape = l.crop_shape\n",
    "        if crop_shape is None:\n",
    "            raise AttributeError(f\"Could not find crop_shape attribute for layer {self.get_attr('name')}\")\n",
    "        else:\n",
    "            self.set_attr('crop_rows', crop_shape[0])\n",
    "            self.set_attr('crop_cols', crop_shape[1])\n",
    "\n",
    "        num_channels = inp_0.shape[-1]\n",
    "        num_channels_name = inp_0.dim_names[-1]\n",
    "        num_boxes = NUM_BLOBS\n",
    "        num_boxes_name = \"NUM_BOXES\"\n",
    "\n",
    "        crop_rows = self.get_attr('crop_rows')\n",
    "        crop_cols = self.get_attr('crop_cols')\n",
    "\n",
    "        shape = [num_boxes, crop_rows, crop_cols, num_channels]\n",
    "        dim_names = [num_boxes_name, f\"CROP_ROWS_{self.index}\", f\"CROP_COLS_{self.index}\", num_channels_name]\n",
    "        self.add_output_variable(shape, dim_names)\n",
    "\n",
    "hls4ml.converters.register_keras_layer_handler('CropLayer', parse_crop_layer)\n",
    "hls4ml.model.layers.register_layer('CropLayer', HCropLayer)\n",
    "\n",
    "# Templates\n",
    "crop_config_template = \"\"\"struct config{index} : nnet::crop_config {{\n",
    "    static const unsigned in_height = {in_height};\n",
    "    static const unsigned in_width = {in_width};\n",
    "    static const unsigned n_chan = {n_chan};\n",
    "    static const unsigned n_crop_boxes = {n_crop_boxes};\n",
    "    static const unsigned crop_rows = {crop_rows};\n",
    "    static const unsigned crop_cols = {crop_cols};\n",
    "\\\\\n",
    "}};\\n\"\"\"\n",
    "\n",
    "crop_function_template = \"nnet::crop<{input1_t}, {index_t}, {output_t}, {config}>({input1}, {output});\"\n",
    "\n",
    "crop_include_list = [\"nnet_utils/nnet_crop.h\"]\n",
    "\n",
    "class HCropLayerConfigTemplate(hls4ml.backends.template.LayerConfigTemplate):\n",
    "    def __init__(self):\n",
    "        super().__init__(HCropLayer)\n",
    "        self.template = crop_config_template\n",
    "\n",
    "    def format(self, node):\n",
    "        params = self._default_config_params(node)\n",
    "\n",
    "        image_input = node.get_input_variable(node.inputs[0])\n",
    "        params['in_height'] = image_input.shape[0]\n",
    "        params['in_width'] = image_input.shape[1]\n",
    "        params['n_chan'] = image_input.shape[2]\n",
    "\n",
    "        # crop_coords_input = node.get_input_variable(node.inputs[1])\n",
    "        params['n_crop_boxes'] = NUM_BLOBS\n",
    "        params['crop_rows'] = CROP_BOX_ROWS\n",
    "        params['crop_cols'] = CROP_BOX_COLS\n",
    "        return self.template.format(**params)\n",
    "    \n",
    "class HCropLayerFunctionTemplate(hls4ml.backends.template.FunctionCallTemplate):\n",
    "    def __init__(self):\n",
    "        super().__init__(HCropLayer, include_header=crop_include_list)\n",
    "        self.template = crop_function_template\n",
    "\n",
    "    def format(self, node):\n",
    "        \n",
    "        params = {}\n",
    "        params['config'] = f'config{node.index}'\n",
    "        params['input1_t'] = node.get_input_variable(node.inputs[0]).type.name\n",
    "        params['output_t'] = node.get_output_variable().type.name\n",
    "        params['input1'] = node.get_input_variable(node.inputs[0]).name\n",
    "        params['output'] = node.get_output_variable().name\n",
    "\n",
    "        image_input = node.get_input_variable(node.inputs[0])\n",
    "        in_height = image_input.shape[0]\n",
    "        in_width = image_input.shape[1]\n",
    "        n_chan = image_input.shape[2]\n",
    "        max_idx = in_height*in_width*n_chan\n",
    "        params['index_t'] = f'ap_uint<{int(np.ceil(np.log2(max_idx)))}>'\n",
    "\n",
    "        return self.template.format(**params)\n",
    "    \n",
    "backend = hls4ml.backends.get_backend('Vivado')\n",
    "backend.register_template(HCropLayerConfigTemplate)\n",
    "backend.register_template(HCropLayerFunctionTemplate)\n",
    "p = Path(\"nnet_crop.h\")\n",
    "backend.register_source(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_coords_hard = np.array([0.38787913, 0.37316015, 0.86787915, 0.67316014], dtype=np.float32)\n",
    "\n",
    "y1 = int(crop_coords_hard[0]*IMAGE_SIZE[0])\n",
    "x1 = int(crop_coords_hard[1]*IMAGE_SIZE[1])\n",
    "y2 = int(crop_coords_hard[2]*IMAGE_SIZE[0])\n",
    "x2 = int(crop_coords_hard[3]*IMAGE_SIZE[1])\n",
    "\n",
    "padded_normalized_images = np.zeros((normalized_images.shape[0], IMAGE_SIZE[0], IMAGE_SIZE[1]), dtype=np.float32)\n",
    "padded_normalized_images[:, y1:y2, x1:x2] = normalized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataSet:\n",
    "batch_size = 1000\n",
    "\n",
    "with tf.device('CPU'):\n",
    "    padded_dataset = tf.data.Dataset.from_tensor_slices(padded_normalized_images)\n",
    "    padded_dataset = padded_dataset.shuffle(padded_normalized_images.shape[0], reshuffle_each_iteration=True)\n",
    "    padded_dataset = padded_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SHAPE = normalized_images.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"crop_layer\" (type CropLayer).\n\nin user code:\n\n    File \"/tmp/ipykernel_48106/449642735.py\", line 20, in call  *\n        cropped_images = tf.image.crop_and_resize(\n\n    ValueError: Shape must be rank 4 but is rank 3 for '{{node crop_layer/CropAndResize}} = CropAndResize[T=DT_FLOAT, extrapolation_value=0, method=\"bilinear\"](Placeholder, crop_layer/Reshape_1, crop_layer/Repeat_1/Reshape_1, crop_layer/CropAndResize/crop_size)' with input shapes: [?,100,160], [?,4], [?], [2].\n\n\nCall arguments received by layer \"crop_layer\" (type CropLayer):\n  • inputs=tf.Tensor(shape=(None, 100, 160), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39mx)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 18\u001b[0m model2_wcrop \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model_w_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCROP_SHAPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# model2_wcrop = tf.keras.Sequential(\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     [\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#         CropLayer(image_shape = IMAGE_SIZE, crop_shape = CROP_SHAPE, num_boxes = 1, batch_input_shape=(None, CROP_BOX_ROWS, CROP_BOX_COLS, 1)),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     ]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model2_wcrop\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mcustom_weighted_mse_loss)\n",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m, in \u001b[0;36mbuild_model_w_crop\u001b[0;34m(image_shape, crop_size, num_boxes)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model_w_crop\u001b[39m(image_shape, crop_size, num_boxes):\n\u001b[1;32m      5\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39mimage_shape)\n\u001b[0;32m----> 6\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mCropLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[1;32m      8\u001b[0m     x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mReLU()(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/rheed_hls4ml_dev/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filei87ka3jd.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m box_indices \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrange, (ag__\u001b[38;5;241m.\u001b[39mld(batch_size),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m box_indices \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrepeat, (ag__\u001b[38;5;241m.\u001b[39mld(box_indices),), \u001b[38;5;28mdict\u001b[39m(repeats\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnum_boxes), fscope)\n\u001b[0;32m---> 17\u001b[0m cropped_images \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_and_resize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_boxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"crop_layer\" (type CropLayer).\n\nin user code:\n\n    File \"/tmp/ipykernel_48106/449642735.py\", line 20, in call  *\n        cropped_images = tf.image.crop_and_resize(\n\n    ValueError: Shape must be rank 4 but is rank 3 for '{{node crop_layer/CropAndResize}} = CropAndResize[T=DT_FLOAT, extrapolation_value=0, method=\"bilinear\"](Placeholder, crop_layer/Reshape_1, crop_layer/Repeat_1/Reshape_1, crop_layer/CropAndResize/crop_size)' with input shapes: [?,100,160], [?,4], [?], [2].\n\n\nCall arguments received by layer \"crop_layer\" (type CropLayer):\n  • inputs=tf.Tensor(shape=(None, 100, 160), dtype=float32)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_model_w_crop(image_shape, crop_size, num_boxes):\n",
    "    inputs = layers.Input(shape=image_shape)\n",
    "    x = CropLayer(image_shape, crop_size, num_boxes)(inputs)\n",
    "    x = layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='valid')(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(pool_size=4, strides=4)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=98, activation='relu')(x)\n",
    "    x = layers.Dense(units=5, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "model2_wcrop = build_model_w_crop(IMAGE_SIZE, CROP_SHAPE, 1)\n",
    "# model2_wcrop = tf.keras.Sequential(\n",
    "#     [\n",
    "#         CropLayer(image_shape = IMAGE_SIZE, crop_shape = CROP_SHAPE, num_boxes = 1, batch_input_shape=(None, CROP_BOX_ROWS, CROP_BOX_COLS, 1)),\n",
    "#         tf.keras.layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='valid', input_shape=(48, 48, 1)),\n",
    "#         tf.keras.layers.ReLU(),\n",
    "#         tf.keras.layers.MaxPool2D(pool_size=4, strides=4),\n",
    "\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(units=98, activation='relu'),\n",
    "#         tf.keras.layers.Dense(units=5, activation='softmax')\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "model2_wcrop.compile(optimizer='adam', loss=custom_weighted_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.CropLayer at 0x7f7d481a93f0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f7d481a98d0>,\n",
       " <keras.layers.activation.relu.ReLU at 0x7f7d481aa380>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f7d481a83a0>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x7f7d481a87c0>,\n",
       " <keras.layers.core.dense.Dense at 0x7f7d481a9cf0>,\n",
       " <keras.layers.core.dense.Dense at 0x7f7d481aa710>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_wcrop.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Loop\n",
    "# train_model = YES\n",
    "# save_model = YES\n",
    "# load_model = NO\n",
    "\n",
    "# if train_model:\n",
    "#     best_loss = float('inf')\n",
    "#     num_epochs = 1\n",
    "#     lr = 0.0001\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#     n = 1\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         if epoch % 10 == 0:\n",
    "#             n += 0.1\n",
    "\n",
    "#         for image_batch in tqdm(dataset): \n",
    "#             image_batch = tf.expand_dims(image_batch, axis=3) # (batch_size, height, width, channels)\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 embedding = model2_wcrop(image_batch)\n",
    "#                 unscaled_param = tf.constant(embedding * output_scaler.var_ ** 0.5 + output_scaler.mean_)\n",
    "#                 final = generate_guassian(unscaled_param, (48,48))\n",
    "#                 loss = custom_weighted_mse_loss(image_batch, final, n)\n",
    "#             grads = tape.gradient(loss, model2_wcrop.trainable_variables)\n",
    "#             optimizer.apply_gradients(zip(grads, model2_wcrop.trainable_variables))\n",
    "\n",
    "#             running_loss += loss.numpy()\n",
    "#         average_loss = running_loss / len(dataset)\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "\n",
    "# if save_model:\n",
    "#     model2_wcrop.save(\"GaussianModel2_wcrop.keras\")\n",
    "\n",
    "# if load_model:\n",
    "#     pass\n",
    "\n",
    "# model2_wcrop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: crop_layer_input, layer type: InputLayer, input shapes: [[None, 48, 48, 1]], output shape: [None, 48, 48, 1]\n",
      "input_names = None\n",
      "input_shapes = [[None, 48, 48, 1]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate the configuration from the Keras model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m config2_wcrop \u001b[38;5;241m=\u001b[39m \u001b[43mhls4ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_from_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2_wcrop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print out the config to debug and check for unwanted settings\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(config2_wcrop)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Attempt conversion on simplified model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m hls_model2_wcrop \u001b[38;5;241m=\u001b[39m hls4ml\u001b[38;5;241m.\u001b[39mconverters\u001b[38;5;241m.\u001b[39mconvert_from_keras_model(model2_wcrop, \n\u001b[1;32m      9\u001b[0m                                                               hls_config\u001b[38;5;241m=\u001b[39mconfig2_wcrop, \n\u001b[1;32m     10\u001b[0m                                                               output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_2_wcrop/hls4ml_prj\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     11\u001b[0m                                                               part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxcu250-figd2104-2L-e\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     12\u001b[0m                                                               io_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio_stream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/utils/config.py:165\u001b[0m, in \u001b[0;36mconfig_from_keras_model\u001b[0;34m(model, granularity, backend, default_precision, default_reuse_factor, max_precision)\u001b[0m\n\u001b[1;32m    161\u001b[0m     model_arch \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(model\u001b[38;5;241m.\u001b[39mto_json())\n\u001b[1;32m    163\u001b[0m reader \u001b[38;5;241m=\u001b[39m hls4ml\u001b[38;5;241m.\u001b[39mconverters\u001b[38;5;241m.\u001b[39mKerasModelReader(model)\n\u001b[0;32m--> 165\u001b[0m layer_list, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mhls4ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_keras_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_layer_config\u001b[39m(layer):\n\u001b[1;32m    168\u001b[0m     cls_name \u001b[38;5;241m=\u001b[39m layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/RHEED/hls4ml/hls4ml/converters/keras_to_hls.py:291\u001b[0m, in \u001b[0;36mparse_keras_model\u001b[0;34m(model_arch, reader)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     input_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m layer, output_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_handlers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeras_class\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer name: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, layer type: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, input shapes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, output shape: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    295\u001b[0m         layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m], input_shapes, output_shape\n\u001b[1;32m    296\u001b[0m     )\n\u001b[1;32m    297\u001b[0m )\n\u001b[1;32m    298\u001b[0m layer_list\u001b[38;5;241m.\u001b[39mappend(layer)\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mparse_crop_layer\u001b[0;34m(keras_layer, input_names, input_shapes, data_reader)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shapes = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shapes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCropLayer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m keras_layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_layer[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeras_layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m crop_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m data_reader\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'"
     ]
    }
   ],
   "source": [
    "# Generate the configuration from the Keras model\n",
    "config2_wcrop = hls4ml.utils.config_from_keras_model(model2_wcrop)\n",
    "\n",
    "# Print out the config to debug and check for unwanted settings\n",
    "# print(config2_wcrop)\n",
    "\n",
    "# Attempt conversion on simplified model\n",
    "hls_model2_wcrop = hls4ml.converters.convert_from_keras_model(model2_wcrop, \n",
    "                                                              hls_config=config2_wcrop, \n",
    "                                                              output_dir='model_2_wcrop/hls4ml_prj', \n",
    "                                                              part='xcu250-figd2104-2L-e', \n",
    "                                                              io_type=\"io_stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compare predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheed_hls4ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
